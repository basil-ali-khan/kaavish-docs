
% This chapter provides important artifacts related to design of our project.

% \section{Software Design}

% This section presents the UML class diagram and gives a brief description of each class in our system. Attributes and methods of each class and relationship among classes are clearly presented.

% % Your report will contain ONE of the following 2 sections.

% \section{Data Design}

% This section presents the structure of our database that caters to persistent data storage in our project. The structure is shown as a normalized data model for relational databases. It clearly shows entities, attributes, relationships with their cardinalities, and primary and foreign keys. We have used DB designer (or any other similar data modeling tool) to build our data model.

 
% \section{Technical Details}

% Our project does not have persistent data so we have no ERD. Instead we exaplin here the technical details of the algortihsm we use. These include the inputs and the outputs, how and where these algorothms fit in our tool chain, the techniques used in these algorithms, etc.
% \usepackage{float}
\section{Project Architecture}
Insert Project architecture picture here
\section{Data Model}

This section defines the logical data model used in the system. The architecture follows a 
hybrid design where all raw logs are stored as JSONB documents in PostgreSQL, while all 
derived AI outputs (anomalies, RCA results, predictions, remediation actions, and human 
feedback) are stored in normalized relational tables. This ensures both flexibility for 
unstructured logs and strong consistency for AI processing workflows.

\subsection{Logs}

The \texttt{logs} table stores all raw and normalized switch logs.  
It is the primary ingestion point and the parent entity for anomaly detection.

\begin{table}[H]
\centering
\begin{tabular}{|p{4cm}|p{3cm}|p{7cm}|}
\hline
\textbf{Field} & \textbf{Type} & \textbf{Description} \\ \hline
id & BIGSERIAL PK & Unique log identifier \\ \hline
ingest\_time & TIMESTAMPTZ & Time when log was ingested into the system \\ \hline
raw\_log & JSONB & Full raw log payload \\ \hline
status & TEXT & Status extracted from log (INFO/ERROR/etc.) \\ \hline
timestamp & TIMESTAMPTZ & Timestamp from the switch log \\ \hline
transaction\_id & TEXT & Transaction identifier (nullable) \\ \hline
process\_id & TEXT & Process or module identifier \\ \hline
\end{tabular}
\end{table}

\paragraph{JSONB Log Structure}
The \texttt{raw\_log} JSONB object contains the full unmodified log entry received from Logstash.  
A typical structure is:

\begin{verbatim}
{
  "timestamp": "2025-03-01T14:20:10Z",
  "status": "ERROR",
  "processID": "4173",
  "transactionID": "895299881",
  "filename": "switch_core.cpp",
  "function": "processDebit",
  "message": "Timeout waiting for issuer response"
}
\end{verbatim}

\subsection{Anomaly Events}

The \texttt{anomaly\_events} table stores anomalies detected by the agent.

\begin{table}[H]
\centering
\begin{tabular}{|p{4cm}|p{3cm}|p{7cm}|}
\hline
id & BIGSERIAL PK & Unique anomaly identifier \\ \hline
log\_id & BIGINT FK → logs(id) & Log entry where anomaly occurred \\ \hline
detected\_at & TIMESTAMPTZ & Timestamp of anomaly detection \\ \hline
anomaly\_type & TEXT & Type/category of anomaly \\ \hline
severity & TEXT & Severity level (Low/Medium/High/Critical) \\ \hline
summary & TEXT & Short description of the issue \\ \hline
status & TEXT & Processing status \\ \hline
predicted & BOOLEAN & Flag indicating if anomaly was predicted \\ \hline
\end{tabular}
\end{table}

\subsection{RCA Results}

The \texttt{rca\_results} table stores root-cause analysis generated by the agent.

\begin{table}[H]
\centering
\begin{tabular}{|p{4cm}|p{3cm}|p{7cm}|}
\hline
id & BIGSERIAL PK & Unique RCA identifier \\ \hline
anomaly\_id & BIGINT FK → anomaly\_events(id) & Related anomaly \\ \hline
agent\_version & TEXT & Agent version used \\ \hline
root\_cause & TEXT & Probable underlying cause \\ \hline
confidence & FLOAT & Confidence score \\ \hline
suggested\_fix & TEXT & Recommended fix \\ \hline
created\_at & TIMESTAMPTZ & Creation timestamp \\ \hline
\end{tabular}
\end{table}

\subsection{RCA Causes}

The \texttt{rca\_causes} table stores multiple possible causes with confidence values.

\begin{table}[H]
\centering
\begin{tabular}{|p{4cm}|p{3cm}|p{7cm}|}
\hline
id & BIGSERIAL PK & Unique cause identifier \\ \hline
rca\_result\_id & BIGINT FK → rca\_results(id) & Related RCA result \\ \hline
cause & TEXT & Explanation of cause \\ \hline
confidence & FLOAT & Confidence score \\ \hline
\end{tabular}
\end{table}

\subsection{Anomaly Predictions}

The \texttt{anomaly\_predictions} table stores predictions of future anomalies.

\begin{table}[H]
\centering
\begin{tabular}{|p{4cm}|p{3cm}|p{7cm}|}
\hline
id & BIGSERIAL PK & Unique prediction identifier \\ \hline
anomaly\_id & BIGINT FK → anomaly\_events(id) (nullable) & Linked anomaly if prediction later materialized \\ \hline
predicted\_at & TIMESTAMPTZ & Time prediction was produced \\ \hline
predicted\_for & TIMESTAMPTZ & Time window for predicted event \\ \hline
probability & FLOAT & Likelihood score \\ \hline
anomaly\_type & TEXT & Predicted category of anomaly \\ \hline
reason & TEXT & Reason behind prediction \\ \hline
model\_version & TEXT & Prediction model version \\ \hline
\end{tabular}
\end{table}

\subsection{Remediation Actions}

The \texttt{remediation\_actions} table stores the actions suggested by the agent.

\begin{table}[H]
\centering
\begin{tabular}{|p{4cm}|p{3cm}|p{7cm}|}
\hline
id & BIGSERIAL PK & Unique remediation identifier \\ \hline
anomaly\_id & BIGINT FK → anomaly\_events(id) & Target anomaly \\ \hline
suggested\_at & TIMESTAMPTZ & Timestamp when action was suggested \\ \hline
action & TEXT & Proposed remediation \\ \hline
reason & TEXT & Why the action is recommended \\ \hline
confidence & FLOAT & Agent confidence \\ \hline
status & TEXT & Approved/Rejected/Pending \\ \hline
\end{tabular}
\end{table}

\subsection{Validation Feedback}

The \texttt{validation\_feedback} table stores human-in-the-loop decisions.

\begin{table}[H]
\centering
\begin{tabular}{|p{4cm}|p{3cm}|p{7cm}|}
\hline
id & BIGSERIAL PK & Unique feedback identifier \\ \hline
remediation\_id & BIGINT FK → remediation\_actions(id) & Related remediation \\ \hline
user\_id & BIGINT (nullable) & Operator who approved/rejected \\ \hline
action & TEXT & Approval decision \\ \hline
comments & TEXT & Operator remarks \\ \hline
timestamp & TIMESTAMPTZ & Feedback timestamp \\ \hline
\end{tabular}
\end{table}

\subsection{Users}

\begin{table}[H]
\centering
\begin{tabular}{|p{4cm}|p{3cm}|p{7cm}|}
\hline
id & BIGSERIAL PK & Unique user identifier \\ \hline
username & TEXT & Login username \\ \hline
fullname & TEXT & Full name \\ \hline
role & TEXT & Role (Admin/Operator/Viewer) \\ \hline
email & TEXT & Email address \\ \hline
\end{tabular}
\end{table}


\subsection{Relationship Summary}

\begin{itemize}
    \item One \textbf{log} may generate zero or one \textbf{anomaly\_events}.
    \item One \textbf{anomaly\_event} must have one \textbf{RCA result}.
    \item One \textbf{RCA result} may have multiple \textbf{RCA causes}.
    \item One \textbf{anomaly\_event} may produce zero or many \textbf{predictions}.
    \item One \textbf{anomaly\_event} may produce multiple \textbf{remediation actions}.
    \item Each \textbf{remediation action} may have multiple \textbf{validation feedback} items.
    \item Users provide feedback but are optional (nullable FK).
\end{itemize}



\section{Data Pipeline}

This section describes the end-to-end data flow of the system, from the generation of raw payment-switch logs to anomaly detection, root-cause analysis (RCA), prediction, and dashboard visualization.

\subsection{Log Ingestion}
Raw logs are continuously generated by the payment switch, core banking system, and backend transaction services. Each log record typically contains:
\begin{itemize}
    \item timestamp
    \item status
    \item processID
    \item transactionID
    \item filename
    \item function
    \item message
\end{itemize}

These logs are forwarded to Logstash.

\subsection{Log Parsing and Normalization (Logstash)}
Logstash processes the incoming logs by:
\begin{enumerate}
    \item Parsing fields
    \item Cleaning and normalizing the structure
    \item Converting logs into a consistent JSON document format
\end{enumerate}

Logstash then outputs the structured logs into two parallel paths:
\begin{itemize}
    \item \textbf{Pipeline A:} Structured logs sent to PostgreSQL (Document Store)
    \item \textbf{Pipeline B:} Logs streamed into Kafka for real-time processing
\end{itemize}

\subsection{Persistent Log Storage (PostgreSQL Document DB)}
Processed logs are stored in PostgreSQL as JSONB documents. This provides:
\begin{itemize}
    \item Historical log retention
    \item Fast querying and auditing
    \item Reliable storage for RCA and prediction modules
\end{itemize}

Only logs are stored as JSONB; all other agent-generated data is stored in relational tables.

\subsection{Real-Time Log Streaming (Kafka)}
In parallel, Logstash streams logs to Kafka, which serves as the system's real-time backbone. Kafka enables:
\begin{itemize}
    \item High-throughput ingestion
    \item Real-time delivery of logs to the agent
    \item Continuous feed of live metrics to the dashboard
\end{itemize}

\subsection{Agent Processing}
The agent consumes logs from Kafka and performs three primary functions:

\subsubsection{Anomaly Detection}
The agent identifies deviations in normal log patterns and flags anomalous system events. Detected anomalies are persisted in PostgreSQL.

\subsubsection{Root Cause Analysis (RCA)}
Upon detecting an anomaly, the agent analyzes surrounding contextual logs, identifies potential causes, and stores RCA results in PostgreSQL.

\subsubsection{Future Anomaly Prediction}
Using historical data stored in PostgreSQL, the agent predicts upcoming failures or system instabilities. Predictions are also written to PostgreSQL.

\subsection{Human-in-the-Loop Validation}
When the agent suggests remediation actions, they are routed to a human operator for approval or rejection.  
Operator feedback is stored in the validation\_feedback repository to improve future decision-making.

\subsection{Dashboard Visualization}
The dashboard integrates two data sources:
\begin{itemize}
    \item \textbf{Real-time metrics from Kafka} for live system monitoring
    \item \textbf{Historical data from PostgreSQL} for anomaly, RCA, and prediction history
\end{itemize}

This provides operators with both immediate insights and long-term visibility into system behavior.

\subsection*{Pipeline Summary}
\textbf{Raw Logs} $\rightarrow$ \textbf{Logstash} $\rightarrow$ \{ \textbf{PostgreSQL} + \textbf{Kafka} \} $\rightarrow$ \textbf{Agent} $\rightarrow$ \{ \textbf{Anomalies, RCA, Predictions} \} $\rightarrow$ \textbf{Dashboard}


\section{Technical Details}

\textbf{Solution Overview:} \\
The system is designed for automated anomaly detection, root cause analysis (RCA), remediation, and feedback collection. It integrates monitoring, analytics, and AI-driven decision-making to streamline incident response.\\

\textbf{Technology Stack:}
\begin{itemize}
    \item \textbf{Frontend:} React 18.3.1, HTML5, CSS3, TypeScript (optional)
    \item \textbf{UI / Styling:} Tailwind CSS 3.4.13, @headlessui/react 1.7.17, @heroicons/react 2.1.1, clsx 2.1.1
    \item \textbf{Backend:} Python
    \item \textbf{Database:} PostgreSQL for structured/persistent storage; optionally Redis for agent context storage
    \item \textbf{Message Queue / Event Bus:} Kafka for log/event streaming
    \item \textbf{Multi-Agent Frameworks:} \\
The system will leverage LangGraph, LangChain, and LangSmith to implement and orchestrate intelligent multi-agent workflows:

\begin{itemize}
    \item \textbf{LangGraph:} Provides a graph-based framework to define, connect, and manage multiple agents, enabling structured interactions and task delegation between them.
    \item \textbf{LangChain:} Handles agent reasoning, context management, and chaining of prompts/actions, allowing agents to perform sequential or dependent tasks with memory of prior interactions.
    \item \textbf{LangSmith:} Provides evaluation, tracking, and orchestration tools for agent workflows, allowing testing, monitoring, and logging of agent decisions and actions for debugging and optimization.
\end{itemize}

\textbf{Usage in System:} \\
These frameworks will coordinate agents responsible for analyzing logs, identifying anomalies, generating remediation suggestions, and summarizing results using pre-trained LLMs. Redis may be used to store agent context and intermediate states, while Kafka streams events to the agents in real time. The combination allows a modular, scalable, and observable multi-agent AI system without requiring custom AI model training.

    \item \textbf{Orchestration / Demo Workflow:} n8n (for demo purposes)
    \item \textbf{Monitoring / Logging:} ELK Stack (Elasticsearch, Logstash, Kibana)
\end{itemize}

\textbf{AI / ML Components:}
\begin{itemize}
    \item \textbf{Anomaly Detection / Prediction:} Using existing models (no new model training)
    \item \textbf{LLMs / NLP:} GPT-family or similar pre-trained models for log summarization and remediation suggestions
    \item \textbf{Model Deployment:} Hugging Face Transformers
\end{itemize}
